# SonarCloud analysis for this C repo
# -----------------------------------------------------------------------------
# What this does:
#   1) Builds the code under Sonar's Build Wrapper (produces compile_commands.json)
#   2) Runs the SonarCloud scanner
#   3) Waits for analysis to finish, pulls issues from SonarCloud's API,
#      converts to SARIF, uploads as an artifact
#   4) Publishes the SARIF to GitHub Code Scanning in a separate, least-privilege job
#
# Why it looks this way (key principles):
#   • Default-deny permissions – read-only by default; elevate only in the upload job
#   • Pinned actions – commit SHAs, not floating tags, for reproducibility and tamper-resistance
#   • Hardened egress – audit (or block) network calls to limit exfiltration risk
#   • Concurrency – cancel superseded runs to save time and money
#   • Timeouts – fail fast instead of hanging for hours
#   • Scoped checkout credentials – don't persist the GitHub token into the repo
#   • Clear triggers – run only when relevant files change
#
# Prereqs in the repo:
#   • sonar-project.properties with:
#       sonar.projectKey=<your_sonarcloud_project_key>
#       sonar.organization=<your_sonarcloud_org_key>
#       sonar.sources=.
#   • A file named main.c at repo root (adjust paths/commands if different)
#
# Secret needed:
#   • SONAR_TOKEN (Actions → Secrets) – a SonarCloud user token with access to the project
#
# CMake-only edits in the workflow. We inject enrichment flags via `-D CMAKE_*_FLAGS_<CONFIG>=...`
# at configure time, while running the actual build under the Sonar build-wrapper so it still emits
# `compile_commands.json`. We can also keep strict/experimental builds as non-gating extras.
#
# Notes:
# - Your CMake includes CodeCoverage.cmake which appends `--coverage`; that’s heavy. We keep it out
#   of the analyzer pass by running the analyzer directly via gcc for C files only.

name: SonarCloud (C - synopsis)

on:
  push:
    branches: [ "main" ]
    # Trigger only when C sources, the workflow, or Sonar properties change
    paths:
      - '**/*.c'
      - '**/*.h'
      - 'sonar-project.properties'
      - '.github/workflows/sonarcloud.yml'
  pull_request:
    branches: [ "main" ]
    types: [opened, synchronize, reopened]
    # paths:
    #  - '**/*.c'
    #  - '**/*.h'
    #  - 'sonar-project.properties'
    #  - '.github/workflows/sonarcloud.yml'
  # workflow_dispatch:

# Cancel older in-flight runs of the same ref. Avoids CI pile-ups on active PRs.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref || github.run_id }}
  cancel-in-progress: true

# Default-deny: grant only read by default at the workflow level.
permissions:
  contents: read

jobs:
  # Job 1: build + scan + export SARIF (no write scopes here)
  build-and-scan:
    name: Build and analyze
    runs-on: ubuntu-24.04            # Explicit runner version for reproducibility
    timeout-minutes: 45              # A little headroom
    permissions:
      contents: read                 # Read-only; no secret write scopes here
    env:
      BW_OUT: bw-out                 # Directory where the Build Wrapper writes output

    steps:
      # Lock down outbound network. Start in audit mode; switch to block + allowlist later.
      - name: Harden runner (audit egress)
        uses: step-security/harden-runner@ec9f2d5744a09debf3a187a3f4f675c53b671911 # v2.13.0
        with:
          egress-policy: audit
          # When the workflow is stable, prefer:
          # egress-policy: block
          # allowed-endpoints: >
          #   sonarcloud.io
          #   api.sonarcloud.io
          #   binaries.sonarsource.com
          #   github.com
          #   api.github.com
          #   uploads.github.com
          #   objects.githubusercontent.com

      # Do not persist the GITHUB_TOKEN into the repo (reduces risk in later steps)
      - name: Checkout (no persisted creds)
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5
        with:
          fetch-depth: 0              # Full history improves blame/issue precision
          persist-credentials: false  # Do not leave a token in .git/config

      # Install Sonar's Build Wrapper (wraps compiler and emits compile_commands.json)
      - name: Install Build Wrapper
        uses: SonarSource/sonarqube-scan-action/install-build-wrapper@1a6d90ebcb0e6a6b1d87e37ba693fe453195ae25 # v5.3.1
        # For SonarCloud, no SONAR_HOST_URL env is required; action defaults to sonarcloud.io

      # Build Wrapper captures the compile so the analyzer knows exactly how files were built
      # Compile under the Build Wrapper. This produces BW_OUT/compile_commands.json that the C analyzer needs.
      # NOTE: We inject extra analysis/hardening flags via CMake *build-type* variables (no repo edits).
            # Build Wrapper captures the compile so the analyzer knows exactly how files were built
      # Compile under the Build Wrapper. This produces BW_OUT/compile_commands.json that the C analyzer needs.
      # NOTE: We inject extra analysis/hardening flags via CMake *build-type* variables (no repo edits).
            # Build Wrapper captures the compile so the analyzer knows exactly how files were built
      # Compile under the Build Wrapper. This produces BW_OUT/compile_commands.json that the C/C++ analyzer needs.
      # We now do TWO wrapped builds (NDEBUG + DEBUG) to exercise more code paths,
      # and then a bounded header-probe pass to pull header-defined code into the DB.
      - name: Configure & build with CMake under Build Wrapper (NDEBUG + DEBUG + header-probe)
        run: |
          set -euxo pipefail
          mkdir -p "${BW_OUT}"

          # Optional: Ninja makes multi-file CMake builds faster and more reproducible.
          if ! command -v ninja >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y ninja-build
          fi

          # Flags to enrich static analysis signals and harden builds (lenient in wrapped builds).
          BASE_CFLAGS="-O2 -g -D_FORTIFY_SOURCE=3 -fstack-protector-strong -fPIE"
          HARDEN_WARNINGS_WRAP="-Wall -Wextra -Wformat=2 -Wformat-security -Wconversion -Wshadow -Wduplicated-cond -Wlogical-op -fno-common"
          LDFLAGS="-Wl,-z,relro,-z,now -pie"
          RELAXED_FLAGS="-Wno-error"

          # Neutralize any coverage flags your CMake adds (saves RAM/CPU under the wrapper).
          CANCEL_COVERAGE="-fno-profile-arcs -fno-test-coverage"

          cmake_config() {
            # $1 = build dir; $2 = extra C/CXX defines (e.g., -DNDEBUG=1 or -DDEBUG=1)
            cmake -S . -B "$1" -G Ninja \
              -DCMAKE_BUILD_TYPE=RelWithDebInfo \
              -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
              -DCMAKE_COMPILE_WARNING_AS_ERROR=OFF \
              -DCMAKE_C_FLAGS_RELWITHDEBINFO="${BASE_CFLAGS} ${HARDEN_WARNINGS_WRAP} ${RELAXED_FLAGS} ${CANCEL_COVERAGE} $2" \
              -DCMAKE_CXX_FLAGS_RELWITHDEBINFO="${BASE_CFLAGS} ${HARDEN_WARNINGS_WRAP} ${RELAXED_FLAGS} ${CANCEL_COVERAGE} $2" \
              -DCMAKE_EXE_LINKER_FLAGS_RELWITHDEBINFO="${LDFLAGS}"
          }

          # -------------------------------
          # Wrapped build #1 (NDEBUG path)
          # -------------------------------
          build-wrapper-linux-x86-64 --out-dir "${BW_OUT}" bash -lc "
            cmake_config build-rd '-DNDEBUG=1';
            cmake --build build-rd --target all -j2
          "

          # -------------------------------
          # Wrapped build #2 (DEBUG path)
          # -------------------------------
          build-wrapper-linux-x86-64 --out-dir "${BW_OUT}" bash -lc "
            cmake_config build-dbg '-DDEBUG=1';
            cmake --build build-dbg --target all -j2
          "

          # -----------------------------------------
          # Header-probe under wrapper (bounded)
          # -----------------------------------------
          # Goal: add compile DB entries for header-defined code (inline/template),
          # without dragging in third-party or gargantuan files.
          # We compile each header as a dummy TU with -x c or -x c++ based on extension.
          # Time-box each compile to avoid stalls; never fail the job.
          probe_one_header() {
            hdr="$1"
            case "$hdr" in
              */_deps/*|*/third_party/*|*/external/*|*/extern/*|*/.venv/*) return 0 ;;
              */sqlite3.h|*/sqlite3ext.h|*/sqlite3.c|*/sqlite3*.hpp) return 0 ;;
            esac
            langflag="-xc"
            [[ "$hdr" =~ \.hh$|\.hpp$|\.hxx$|\.H$ ]] && langflag="-xc++"
            timeout 20s build-wrapper-linux-x86-64 --out-dir "${BW_OUT}" \
              sh -c "gcc -O0 -g ${RELAXED_FLAGS} -I. ${langflag} -include \"$hdr\" -c /dev/null -o /tmp/probe.$$.o" || true
          }

          echo "::group::Header-probe (under wrapper; bounded)"
          # Probe headers in project include/ and src/ trees only.
          mapfile -t HDRS < <(git ls-files -- 'include/**/*.h' 'include/**/*.hpp' 'src/**/*.h' 'src/**/*.hpp' 2>/dev/null || true)
          for h in "${HDRS[@]}"; do
            probe_one_header "$h"
          done
          echo "::endgroup::"
        timeout-minutes: 30



      # Optional secondary builds to enrich static analysis signals (do not gate CI)
      - name: Secondary analysis-only builds (per-file fanalyzer for C, plus ASan/UBSan)
        run: |
          set -euxo pipefail

          echo "::group::Per-file C fanalyzer (non-gating, bounded)"
          # Gather C files, exclude tests and the giant amalgamation sqlite3.c
          mapfile -t CFILES < <(git ls-files -- '*.c' ':!:**/test/**' | grep -vE '/sqlite3\.c$' || true)

          # Analyze each file with a per-file timeout and low priority; never gate CI.
          for f in "${CFILES[@]}"; do
            echo "analyze: $f"
            # 90s per file should keep total time sane; adjust if needed.
            timeout 90s nice -n 10 gcc -O0 -g -fanalyzer -Wnull-dereference -c "$f" -o /dev/null || true
          done
          echo "::endgroup::"

          echo "::group::ASan/UBSan CMake build (non-gating)"
          if ! command -v ninja >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y ninja-build
          fi
          cmake -S . -B build-sanitizers -G Ninja \
            -DCMAKE_BUILD_TYPE=RelWithDebInfo \
            -DCMAKE_C_FLAGS_RELWITHDEBINFO='-O1 -g -fno-omit-frame-pointer -fsanitize=address,undefined' \
            -DCMAKE_CXX_FLAGS_RELWITHDEBINFO='-O1 -g -fno-omit-frame-pointer -fsanitize=address,undefined' \
            -DCMAKE_EXE_LINKER_FLAGS_RELWITHDEBINFO='-fsanitize=address,undefined' || true
          cmake --build build-sanitizers --target synopsis synopsis_cli -j2 || true
          echo "::endgroup::"
        timeout-minutes: 10


      # Fail fast if the wrapper didn't produce the expected file
      - name: Verify compile_commands.json exists
        run: |
          set -eux
          ls -l "${BW_OUT}"
          test -s "${BW_OUT}/compile_commands.json"

      # Run the scanner. It reads sonar-project.properties for org/project.
      # We pass the path to compile_commands.json so the CFamily analyzer can do its job.
      - name: SonarCloud scan
        uses: SonarSource/sonarqube-scan-action@1a6d90ebcb0e6a6b1d87e37ba693fe453195ae25 # v5.3.1
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}  # SonarCloud user token; no GitHub write scope needed
        with:
          args: >
            --define sonar.cfamily.compile-commands=${{ env.BW_OUT }}/compile_commands.json
            --define sonar.verbose=true

      # --define sonar.scm.exclusions.disabled=false
      # --define sonar.sourceEncoding=UTF-8
      # --define sonar.ws.timeout=120

      # Convert SonarCloud issues → SARIF so GitHub Code Scanning can ingest them.
      # Why a custom step? SonarCloud doesn’t publish SARIF directly, so we fetch issues via API and transcode.
      - name: Generate SARIF from SonarCloud issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          set -euo pipefail
          python3 - <<'PY'
          # Fetch unresolved SonarCloud issues for the most recent analysis of this project
          # and write a minimal SARIF 2.1.0 file at reports/sonarcloud.sarif
          import base64, json, os, time, urllib.parse, urllib.request, pathlib, sys

          def read_project_key():
              with open("sonar-project.properties", "r", encoding="utf-8") as f:
                  for line in f:
                      line=line.strip()
                      if line.startswith("sonar.projectKey="):
                          return line.split("=",1)[1].strip()
              raise RuntimeError("sonar.projectKey not found in sonar-project.properties")

          def get_json(url, token):
              req = urllib.request.Request(url)
              auth = base64.b64encode((token + ":").encode()).decode()
              req.add_header("Authorization", "Basic " + auth)
              with urllib.request.urlopen(req, timeout=30) as r:
                  return json.load(r)

          base = "https://sonarcloud.io"
          token = os.environ["SONAR_TOKEN"]
          project_key = read_project_key()
          print(f"Project: {project_key}")

          # Wait until the most recent Compute Engine (CE) task has finished
          for _ in range(120):  # ~6 minutes max
              act = get_json(f"{base}/api/ce/activity?component={urllib.parse.quote(project_key)}&onlyCurrents=true&ps=1", token)
              tasks = act.get("tasks", [])
              if tasks:
                  tid = tasks[0]["id"]
                  ce = get_json(f"{base}/api/ce/task?id={tid}", token)
                  status = ce["task"]["status"]
                  print(f"CE status: {status}")
                  if status in ("SUCCESS", "FAILED", "CANCELED"):
                      break
              else:
                  print("No CE tasks yet")
              time.sleep(3)

          # Pull unresolved issues for the current branch/analysis
          p=1; page_size=500; issues=[]
          while True:
              data = get_json(f"{base}/api/issues/search?componentKeys={urllib.parse.quote(project_key)}&resolved=false&ps={page_size}&p={p}", token)
              batch = data.get("issues", [])
              if not batch:
                  break
              issues.extend(batch)
              total = data.get("total", 0)
              if p*page_size >= total:
                  break
              p += 1

          def level(sev):
              return {"BLOCKER":"error","CRITICAL":"error","MAJOR":"warning","MINOR":"note","INFO":"note"}.get(sev,"warning")

          runs = [{
              "tool": {
                  "driver": {
                      "name": "SonarCloud",
                      "informationUri": "https://sonarcloud.io",
                      "rules": []
                  }
              },
              "results": []
          }]

          rule_added = set()
          for it in issues:
              rid = str(it.get("rule","unknown"))
              if rid not in rule_added:
                  runs[0]["tool"]["driver"]["rules"].append({
                      "id": rid,
                      "shortDescription": {"text": (it.get("message","") or "")[:120]}
                  })
                  rule_added.add(rid)
              comp = it.get("component","")
              path = comp.split(":",1)[1] if ":" in comp else comp
              tr = it.get("textRange") or {}
              sl = tr.get("startLine", 1)
              sc = (tr.get("startOffset",0) or 0) + 1
              el = tr.get("endLine", sl)
              ec = max(tr.get("endOffset", sc), sc)
              runs[0]["results"].append({
                  "ruleId": rid,
                  "level": level(it.get("severity","MAJOR")),
                  "message": {"text": it.get("message","")},
                  "locations": [{
                      "physicalLocation": {
                          "artifactLocation": {"uri": path, "uriBaseId": "PROJECT_ROOT"},
                          "region": {"startLine": sl, "startColumn": sc, "endLine": el, "endColumn": ec}
                      }
                  }],
                  "fingerprints": {"sonarIssueKey": it.get("key","")}
              })

          pathlib.Path("reports").mkdir(parents=True, exist_ok=True)
          sarif = {"version":"2.1.0", "$schema":"https://json.schemastore.org/sarif-2.1.0.json", "runs": runs}
          with open("reports/sonarcloud.sarif","w",encoding="utf-8") as f:
              json.dump(sarif, f, ensure_ascii=False)
          print(f"Wrote {len(issues)} issues to reports/sonarcloud.sarif")
          PY
        timeout-minutes: 15

      # Keep an auditable copy of the SARIF. Do not upload secrets.
      - name: Upload SARIF as artifact
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: sonarcloud-sarif
          path: reports/sonarcloud.sarif
          retention-days: 7
          if-no-files-found: error

  # Job 2: upload SARIF to GitHub Code Scanning with the only write scope it needs
  upload-to-code-scanning:
    name: Publish SARIF to Code Scanning
    needs: build-and-scan               # Run only after SARIF is produced
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    permissions:
      contents: read
      security-events: write            # Narrow write scope to this tiny, isolated job
      actions: read
    steps:
      - name: Harden runner (audit egress)
        uses: step-security/harden-runner@ec9f2d5744a09debf3a187a3f4f675c53b671911 # v2.13.0
        with:
          egress-policy: audit

      - name: Download SARIF artifact
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0 # v5.0.0
        with:
          name: sonarcloud-sarif
          path: reports

      - name: Upload SARIF to GitHub Code Scanning
        uses: github/codeql-action/upload-sarif@3c3833e0f8c1c83d449a7478aa59c036a9165498 # v3.29.11
        with:
          sarif_file: reports/sonarcloud.sarif
          category: sonarcloud
